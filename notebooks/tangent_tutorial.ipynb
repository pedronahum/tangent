{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangent: Source-to-Source Automatic Differentiation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pedronahum/tangent/blob/master/notebooks/tangent_tutorial.ipynb)\n",
    "\n",
    "Welcome to this comprehensive tutorial on **Tangent**, a library for automatic differentiation that works by transforming Python source code directly. This makes gradients readable, debuggable, and efficient!\n",
    "\n",
    "## What Makes Tangent Special?\n",
    "\n",
    "- **Source-to-Source**: Transforms Python code directly, making gradients human-readable\n",
    "- **Multi-Backend**: Works with NumPy, TensorFlow, and JAX\n",
    "- **Debuggable**: Generated gradient code can be inspected and stepped through\n",
    "- **Efficient**: No tape overhead at runtime\n",
    "\n",
    "## Tutorial Contents\n",
    "\n",
    "1. Installation & Setup\n",
    "2. Basic Concepts\n",
    "3. NumPy Integration\n",
    "4. TensorFlow 2.x Integration\n",
    "5. JAX Integration\n",
    "6. Advanced Features\n",
    "7. Visualization & Debugging\n",
    "8. Real-World Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, let's install Tangent and the numerical computing libraries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tangent from GitHub and dependencies\n",
    "!pip install git+https://github.com/pedronahum/tangent.git numpy matplotlib\n",
    "\n",
    "# Install optional backends (choose what you need)\n",
    "!pip install jax jaxlib  # For JAX support\n",
    "!pip install tensorflow  # For TensorFlow support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import tangent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Tangent imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Concepts\n",
    "\n",
    "### What is Automatic Differentiation?\n",
    "\n",
    "Automatic differentiation (autodiff) computes derivatives of functions automatically. Unlike:\n",
    "- **Numerical differentiation**: (f(x+h) - f(x))/h (approximate, unstable)\n",
    "- **Symbolic differentiation**: Full symbolic manipulation (can explode in size)\n",
    "\n",
    "Autodiff is:\n",
    "- **Exact**: Computes derivatives to machine precision\n",
    "- **Efficient**: Complexity proportional to original computation\n",
    "\n",
    "### How Tangent Works\n",
    "\n",
    "Tangent transforms your Python function's source code into a new function that computes gradients:\n",
    "\n",
    "```python\n",
    "def f(x):\n",
    "    return x * x\n",
    "```\n",
    "\n",
    "becomes (conceptually):\n",
    "\n",
    "```python\n",
    "def df_dx(x):\n",
    "    return 2 * x\n",
    "```\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# Compute its gradient\n",
    "dsquare = tangent.grad(square)\n",
    "\n",
    "# Test it\n",
    "x_val = 3.0\n",
    "gradient = dsquare(x_val)\n",
    "\n",
    "print(f\"f(x) = xÂ²\")\n",
    "print(f\"f({x_val}) = {square(x_val)}\")\n",
    "print(f\"f'({x_val}) = {gradient}\")\n",
    "print(f\"Expected: {2 * x_val} âœ“\" if abs(gradient - 2 * x_val) < 1e-5 else \"Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Generated Code\n",
    "\n",
    "One of Tangent's superpowers is that you can actually **see** the generated gradient code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the generated gradient code\n",
    "import inspect\n",
    "\n",
    "def polynomial(x):\n",
    "    \"\"\"f(x) = 3xÂ² + 2x + 1\"\"\"\n",
    "    return 3.0 * x * x + 2.0 * x + 1.0\n",
    "\n",
    "# Generate gradient function\n",
    "dpolynomial = tangent.grad(polynomial)\n",
    "\n",
    "# Show the generated code\n",
    "print(\"Original function:\")\n",
    "print(inspect.getsource(polynomial))\n",
    "print(\"\\nGenerated gradient code:\")\n",
    "print(inspect.getsource(dpolynomial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Functions and Gradients\n",
    "\n",
    "Let's create a helper function to visualize functions alongside their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_and_gradient(f, df, x_range, title=\"Function and Gradient\"):\n",
    "    \"\"\"Plot a function and its gradient side by side.\"\"\"\n",
    "    x = np.linspace(*x_range, 200)\n",
    "    y = np.array([f(xi) for xi in x])\n",
    "    dy = np.array([df(xi) for xi in x])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot function\n",
    "    ax1.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "    ax1.set_xlabel('x', fontsize=12)\n",
    "    ax1.set_ylabel('f(x)', fontsize=12)\n",
    "    ax1.set_title('Function', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot gradient\n",
    "    ax2.plot(x, dy, 'r-', linewidth=2, label=\"f'(x)\")\n",
    "    ax2.set_xlabel('x', fontsize=12)\n",
    "    ax2.set_ylabel(\"f'(x)\", fontsize=12)\n",
    "    ax2.set_title('Gradient', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with our polynomial\n",
    "plot_function_and_gradient(\n",
    "    polynomial, \n",
    "    dpolynomial, \n",
    "    (-3, 3),\n",
    "    title=\"Polynomial: f(x) = 3xÂ² + 2x + 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NumPy Integration\n",
    "\n",
    "Tangent works seamlessly with NumPy arrays and operations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_norm_squared(x):\n",
    "    \"\"\"Compute ||x||Â² = sum of xÂ²\"\"\"\n",
    "    return np.sum(x * x)\n",
    "\n",
    "# Gradient of ||x||Â² is 2x\n",
    "dvector_norm_squared = tangent.grad(vector_norm_squared)\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "gradient = dvector_norm_squared(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"||x||Â² = {vector_norm_squared(x)}\")\n",
    "print(f\"âˆ‡||x||Â² = {gradient}\")\n",
    "print(f\"Expected: {2 * x}\")\n",
    "print(f\"Match: {np.allclose(gradient, 2 * x)} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_vector_sum(x):\n",
    "    \"\"\"Compute sum(A @ x) where A is a fixed matrix\"\"\"\n",
    "    A = np.array([[2.0, 1.0, 0.5],\n",
    "                  [1.0, 3.0, 0.7],\n",
    "                  [0.5, 0.7, 4.0]])\n",
    "    return np.sum(np.dot(A, x))\n",
    "\n",
    "# Gradient w.r.t. x\n",
    "df_dx = tangent.grad(matrix_vector_sum)\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "gradient = df_dx(x)\n",
    "\n",
    "A = np.array([[2.0, 1.0, 0.5],\n",
    "              [1.0, 3.0, 0.7],\n",
    "              [0.5, 0.7, 4.0]])\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"A = \\n{A}\")\n",
    "print(f\"\\nâˆ‡_x sum(Ax) = {gradient}\")\n",
    "print(f\"Expected (sum of columns): {np.sum(A, axis=0)}\")\n",
    "print(f\"Match: {np.allclose(gradient, np.sum(A, axis=0))} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_loss(x):\n",
    "    \"\"\"Sum of sigmoid function: sum(1 / (1 + exp(-x)))\"\"\"\n",
    "    return np.sum(1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "dsigmoid_loss = tangent.grad(sigmoid_loss)\n",
    "\n",
    "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "gradient = dsigmoid_loss(x)\n",
    "\n",
    "# Expected: sigmoid(x) * (1 - sigmoid(x))\n",
    "sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n",
    "expected = sigmoid_x * (1.0 - sigmoid_x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"âˆ‡(sigmoid sum) = {gradient}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Match: {np.allclose(gradient, expected)} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow 2.x Integration\n",
    "\n",
    "Tangent works with TensorFlow 2.x in eager execution mode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic TensorFlow Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_quadratic(x):\n",
    "    \"\"\"TensorFlow quadratic: 2xÂ² + 3x + 1\"\"\"\n",
    "    return 2.0 * x * x + 3.0 * x + 1.0\n",
    "\n",
    "dtf_quadratic = tangent.grad(tf_quadratic)\n",
    "\n",
    "x_tf = tf.constant(2.0)\n",
    "gradient = dtf_quadratic(x_tf)\n",
    "\n",
    "print(f\"f(x) = 2xÂ² + 3x + 1\")\n",
    "print(f\"f(2) = {tf_quadratic(x_tf).numpy()}\")\n",
    "print(f\"f'(2) = {gradient.numpy()}\")\n",
    "print(f\"Expected f'(2) = 4*2 + 3 = {4*2 + 3}\")\n",
    "print(f\"Match: {abs(gradient.numpy() - 11) < 1e-5} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Layer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_layer(x, W, b):\n",
    "    \"\"\"Simple neural network layer: sum(tanh(Wx + b))\"\"\"\n",
    "    linear = tf.matmul(tf.reshape(x, [1, -1]), W) + b\n",
    "    activation = tf.tanh(linear)\n",
    "    return tf.reduce_sum(activation)\n",
    "\n",
    "# Compute gradient w.r.t. weights W\n",
    "dlayer_dW = tangent.grad(simple_layer, wrt=(1,))\n",
    "\n",
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "W = tf.constant([[0.5, 0.3], [0.2, 0.7], [0.1, 0.4]])\n",
    "b = tf.constant([0.1, 0.2])\n",
    "\n",
    "gradient = dlayer_dW(x, W, b)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Weight shape: {W.shape}\")\n",
    "print(f\"Gradient shape: {gradient.shape}\")\n",
    "print(f\"\\nGradient w.r.t. W:\\n{gradient.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. JAX Integration\n",
    "\n",
    "Tangent also supports JAX, Google's high-performance numerical computing library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX Array Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_polynomial(x):\n",
    "    \"\"\"JAX polynomial: xÂ³ - 2xÂ² + 3x - 1\"\"\"\n",
    "    return x**3 - 2*x**2 + 3*x - 1\n",
    "\n",
    "djax_polynomial = tangent.grad(jax_polynomial)\n",
    "\n",
    "x_jax = jnp.array(2.0)\n",
    "gradient = djax_polynomial(x_jax)\n",
    "\n",
    "# Expected: 3xÂ² - 4x + 3\n",
    "expected = 3 * x_jax**2 - 4 * x_jax + 3\n",
    "\n",
    "print(f\"f(x) = xÂ³ - 2xÂ² + 3x - 1\")\n",
    "print(f\"f(2) = {jax_polynomial(x_jax)}\")\n",
    "print(f\"f'(2) = {gradient}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Match: {jnp.allclose(gradient, expected)} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX Neural Network Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_relu_network(x):\n",
    "    \"\"\"Simple ReLU network\"\"\"\n",
    "    return jnp.sum(jax.nn.relu(x * x - 1.0))\n",
    "\n",
    "djax_relu_network = tangent.grad(jax_relu_network)\n",
    "\n",
    "x = jnp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "gradient = djax_relu_network(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = sum(relu(xÂ² - 1))\")\n",
    "print(f\"f(x) = {jax_relu_network(x)}\")\n",
    "print(f\"\\nGradient: {gradient}\")\n",
    "print(f\"\\nNote: Gradient is zero where xÂ² - 1 < 0 (ReLU inactive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing JAX Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare activation functions and their gradients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define activation functions as regular functions (not lambdas)\n",
    "def sigmoid_sum(x):\n",
    "    return jnp.sum(jax.nn.sigmoid(x))\n",
    "\n",
    "def tanh_sum(x):\n",
    "    return jnp.sum(jnp.tanh(x))\n",
    "\n",
    "def relu_sum(x):\n",
    "    return jnp.sum(jax.nn.relu(x))\n",
    "\n",
    "def elu_sum(x):\n",
    "    return jnp.sum(jax.nn.elu(x))\n",
    "\n",
    "# Create visualization\n",
    "x = np.linspace(-3, 3, 200)\n",
    "\n",
    "activations = {\n",
    "    'Sigmoid': (sigmoid_sum, 'blue'),\n",
    "    'Tanh': (tanh_sum, 'green'),\n",
    "    'ReLU': (relu_sum, 'red'),\n",
    "    'ELU': (elu_sum, 'orange'),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for idx, (name, (func, color)) in enumerate(activations.items()):\n",
    "    dfunc = tangent.grad(func)\n",
    "    \n",
    "    # Compute values\n",
    "    y = np.array([float(func(jnp.array([xi]))) for xi in x])\n",
    "    dy = np.array([float(dfunc(jnp.array([xi]))) for xi in x])\n",
    "    \n",
    "    # Plot activation\n",
    "    axes[0, idx].plot(x, y, color=color, linewidth=2)\n",
    "    axes[0, idx].set_title(f'{name}', fontweight='bold')\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "    axes[0, idx].set_ylabel('f(x)')\n",
    "    \n",
    "    # Plot gradient\n",
    "    axes[1, idx].plot(x, dy, color=color, linewidth=2, linestyle='--')\n",
    "    axes[1, idx].set_title(f'{name} Gradient', fontweight='bold')\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "    axes[1, idx].set_xlabel('x')\n",
    "    axes[1, idx].set_ylabel(\"f'(x)\")\n",
    "\n",
    "plt.suptitle('JAX Activation Functions and Their Gradients', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Features\n",
    "\n",
    "### Multiple Gradients\n",
    "\n",
    "You can compute gradients with respect to multiple arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate(x, y):\n",
    "    \"\"\"f(x, y) = xÂ²y + xyÂ²\"\"\"\n",
    "    return x * x * y + x * y * y\n",
    "\n",
    "# Compute gradients w.r.t. both x and y\n",
    "dbivariate = tangent.grad(bivariate, wrt=(0, 1))\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "grad_x, grad_y = dbivariate(x, y)\n",
    "\n",
    "# Expected:\n",
    "# âˆ‚f/âˆ‚x = 2xy + yÂ²\n",
    "# âˆ‚f/âˆ‚y = xÂ² + 2xy\n",
    "expected_grad_x = 2 * x * y + y * y\n",
    "expected_grad_y = x * x + 2 * x * y\n",
    "\n",
    "print(f\"f(x, y) = xÂ²y + xyÂ²\")\n",
    "print(f\"f({x}, {y}) = {bivariate(x, y)}\")\n",
    "print(f\"\\nâˆ‚f/âˆ‚x = {grad_x}, expected = {expected_grad_x}\")\n",
    "print(f\"âˆ‚f/âˆ‚y = {grad_y}, expected = {expected_grad_y}\")\n",
    "print(f\"\\nMatch: {abs(grad_x - expected_grad_x) < 1e-5 and abs(grad_y - expected_grad_y) < 1e-5} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multivariate Gradients\n",
    "\n",
    "Let's visualize the gradient field of a 2D function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock function: (1-x)Â² + 100(y-xÂ²)Â²\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "drosenbrock = tangent.grad(rosenbrock, wrt=(0, 1))\n",
    "\n",
    "# Create grid\n",
    "x = np.linspace(-2, 2, 50)\n",
    "y = np.linspace(-1, 3, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute function values\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        Z[j, i] = rosenbrock(X[j, i], Y[j, i])\n",
    "\n",
    "# Compute gradients for quiver plot\n",
    "U = np.zeros_like(X)\n",
    "V = np.zeros_like(Y)\n",
    "for i in range(0, len(x), 3):  # Subsample for clarity\n",
    "    for j in range(0, len(y), 3):\n",
    "        grad_x, grad_y = drosenbrock(X[j, i], Y[j, i])\n",
    "        U[j, i] = -grad_x  # Negative for gradient descent direction\n",
    "        V[j, i] = -grad_y\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Contour plot\n",
    "contour = ax1.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.plot(1, 1, 'r*', markersize=15, label='Minimum (1, 1)')\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('Rosenbrock Function Contours', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient field\n",
    "ax2.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.3)\n",
    "skip = (slice(None, None, 3), slice(None, None, 3))\n",
    "ax2.quiver(X[skip], Y[skip], U[skip], V[skip], \n",
    "           color='red', alpha=0.6, scale=500, width=0.003)\n",
    "ax2.plot(1, 1, 'r*', markersize=15, label='Minimum (1, 1)')\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('y', fontsize=12)\n",
    "ax2.set_title('Gradient Descent Direction', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Rosenbrock Function: f(x,y) = (1-x)Â² + 100(y-xÂ²)Â²', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving Results\n",
    "\n",
    "Sometimes you want both the function value AND its gradient. Use `preserve_result=True`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expensive_function(x):\n",
    "    \"\"\"A function where we want both value and gradient\"\"\"\n",
    "    return np.sum(np.exp(x) * np.sin(x))\n",
    "\n",
    "# Get both gradient and result\n",
    "dexpensive = tangent.grad(expensive_function, preserve_result=True)\n",
    "\n",
    "x = np.array([0.0, 1.0, 2.0])\n",
    "gradient, result = dexpensive(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Function value: {result}\")\n",
    "print(f\"Gradient: {gradient}\")\n",
    "print(f\"\\nVerify function value: {expensive_function(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization & Debugging\n",
    "\n",
    "### Gradient Checking\n",
    "\n",
    "Let's verify our gradients against numerical differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-7):\n",
    "    \"\"\"Compute gradient using finite differences.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        x_minus[i] -= eps\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "def complex_function(x):\n",
    "    \"\"\"A more complex function to test\"\"\"\n",
    "    return np.sum(x**3 - 2*x**2 + np.exp(x/10))\n",
    "\n",
    "dcomplex = tangent.grad(complex_function)\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "tangent_grad = dcomplex(x)\n",
    "numerical_grad = numerical_gradient(complex_function, x)\n",
    "\n",
    "print(\"Gradient Checking\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"\\nTangent gradient:   {tangent_grad}\")\n",
    "print(f\"Numerical gradient: {numerical_grad}\")\n",
    "print(f\"\\nDifference: {np.abs(tangent_grad - numerical_grad)}\")\n",
    "print(f\"Max error: {np.max(np.abs(tangent_grad - numerical_grad)):.2e}\")\n",
    "print(f\"\\nGradients match: {np.allclose(tangent_grad, numerical_grad, atol=1e-5)} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "Let's compare Tangent's performance across different backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\ndef benchmark_gradient(name, grad_func, x, n_iterations=1000):\n    \"\"\"Benchmark gradient computation.\"\"\"\n    # Warmup\n    for _ in range(10):\n        _ = grad_func(x)\n    \n    # Benchmark\n    start = time.time()\n    for _ in range(n_iterations):\n        _ = grad_func(x)\n    elapsed = time.time() - start\n    \n    return elapsed / n_iterations\n\n# Define same function for each backend (using simple polynomial)\ndef numpy_func(x):\n    return np.sum(x**3 - 2*x**2 + x)\n\ndef tf_func(x):\n    return tf.reduce_sum(x**3 - 2*x**2 + x)\n\ndef jax_func(x):\n    return jnp.sum(x**3 - 2*x**2 + x)\n\n# Create gradient functions\ndnumpy = tangent.grad(numpy_func)\ndtf = tangent.grad(tf_func)\ndjax = tangent.grad(jax_func)\n\n# Benchmark\nx_np = np.random.randn(100)\nx_tf = tf.constant(x_np)\nx_jax = jnp.array(x_np)\n\nresults = {\n    'NumPy': benchmark_gradient('NumPy', dnumpy, x_np),\n    'TensorFlow': benchmark_gradient('TensorFlow', dtf, x_tf),\n    'JAX': benchmark_gradient('JAX', djax, x_jax),\n}\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 6))\nbackends = list(results.keys())\ntimes = [results[b] * 1000 for b in backends]  # Convert to milliseconds\n\nbars = ax.bar(backends, times, color=['blue', 'orange', 'green'], alpha=0.7)\nax.set_ylabel('Time per gradient (ms)', fontsize=12)\nax.set_title('Tangent Performance Across Backends', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, time in zip(bars, times):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{time:.3f} ms',\n            ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nBenchmark Results:\")\nprint(\"=\"*50)\nfor backend, time_val in results.items():\n    print(f\"{backend:12s}: {time_val*1000:.3f} ms per gradient\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Examples\n",
    "\n",
    "### Example 1: Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_data = np.random.randn(100, 1)\n",
    "y_data = 3 * X_data + 2 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "def mse_loss(w, b, X, y):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "    predictions = w * X + b\n",
    "    return np.mean((predictions - y) ** 2)\n",
    "\n",
    "# Compute gradients\n",
    "dmse_dw = tangent.grad(mse_loss, wrt=(0,))\n",
    "dmse_db = tangent.grad(mse_loss, wrt=(1,))\n",
    "\n",
    "# Gradient descent\n",
    "w, b = 0.0, 0.0\n",
    "learning_rate = 0.1\n",
    "n_epochs = 50\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Compute gradients\n",
    "    grad_w = dmse_dw(w, b, X_data, y_data)\n",
    "    grad_b = dmse_db(w, b, X_data, y_data)\n",
    "    \n",
    "    # Update parameters\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "    \n",
    "    # Track loss\n",
    "    loss = mse_loss(w, b, X_data, y_data)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal parameters: w = {w:.4f}, b = {b:.4f}\")\n",
    "print(f\"True parameters:  w = 3.0000, b = 2.0000\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Data and fit\n",
    "ax1.scatter(X_data, y_data, alpha=0.5, label='Data')\n",
    "x_line = np.linspace(X_data.min(), X_data.max(), 100)\n",
    "ax1.plot(x_line, w * x_line + b, 'r-', linewidth=2, label=f'Fit: y = {w:.2f}x + {b:.2f}')\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('Linear Regression Result', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curve\n",
    "ax2.plot(loss_history, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax2.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Linear Regression with Tangent', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate binary classification data\nnp.random.seed(42)\nn_samples = 200\nX_class = np.random.randn(n_samples, 2)\ny_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(float).reshape(-1, 1)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef logistic_loss_vec(w1, w2, b, x1_vec, x2_vec, y_vec):\n    \"\"\"Binary cross-entropy loss for logistic regression.\n    \n    Takes pre-extracted column vectors to avoid array indexing issues with Tangent.\n    Uses simple operations that Tangent can handle.\n    \n    Args:\n        w1, w2, b: Model parameters (scalars)\n        x1_vec, x2_vec: Feature columns (1D arrays)\n        y_vec: Labels (1D array)\n    \"\"\"\n    # Compute linear combination\n    z = w1 * x1_vec + w2 * x2_vec + b\n    \n    # Apply sigmoid\n    predictions = sigmoid(z)\n    \n    # Add small epsilon for numerical stability (simpler than np.clip for Tangent)\n    eps = 1e-10\n    predictions = predictions + eps\n    \n    # Binary cross-entropy\n    return -np.mean(y_vec * np.log(predictions) + (1 - y_vec) * np.log(1 - predictions + eps))\n\n# Extract column vectors once (outside the loss function)\nx1_data = X_class[:, 0]\nx2_data = X_class[:, 1]\ny_data = y_class[:, 0]\n\n# Compute gradients separately for each parameter\ndloss_dw1 = tangent.grad(logistic_loss_vec, wrt=(0,))\ndloss_dw2 = tangent.grad(logistic_loss_vec, wrt=(1,))\ndloss_db = tangent.grad(logistic_loss_vec, wrt=(2,))\n\n# Training\nw1, w2, b = 0.0, 0.0, 0.0\nlearning_rate = 0.1\nn_epochs = 100\nloss_history = []\n\nfor epoch in range(n_epochs):\n    # Compute gradients\n    grad_w1 = dloss_dw1(w1, w2, b, x1_data, x2_data, y_data)\n    grad_w2 = dloss_dw2(w1, w2, b, x1_data, x2_data, y_data)\n    grad_b = dloss_db(w1, w2, b, x1_data, x2_data, y_data)\n    \n    # Update parameters\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    b -= learning_rate * grad_b\n    \n    # Track loss\n    loss = logistic_loss_vec(w1, w2, b, x1_data, x2_data, y_data)\n    loss_history.append(loss)\n\nprint(f\"Final parameters: w1 = {w1:.4f}, w2 = {w2:.4f}, b = {b:.4f}\")\n\n# Visualize decision boundary\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Decision boundary\nx1_min, x1_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1\nx2_min, x2_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1\nxx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n                       np.linspace(x2_min, x2_max, 100))\nZ = sigmoid(w1 * xx1 + w2 * xx2 + b)\n\nax1.contourf(xx1, xx2, Z, levels=20, cmap='RdBu', alpha=0.6)\nax1.scatter(X_class[y_class.flatten() == 0, 0], \n           X_class[y_class.flatten() == 0, 1], \n           c='blue', marker='o', label='Class 0', alpha=0.7)\nax1.scatter(X_class[y_class.flatten() == 1, 0], \n           X_class[y_class.flatten() == 1, 1], \n           c='red', marker='s', label='Class 1', alpha=0.7)\nax1.set_xlabel('Feature 1', fontsize=12)\nax1.set_ylabel('Feature 2', fontsize=12)\nax1.set_title('Logistic Regression Decision Boundary', fontsize=14, fontweight='bold')\nax1.legend()\n\n# Loss curve\nax2.plot(loss_history, 'b-', linewidth=2)\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('Cross-Entropy Loss', fontsize=12)\nax2.set_title('Training Loss', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\nplt.suptitle('Logistic Regression with Tangent', fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Neural Network Training (JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-layer neural network with JAX\n",
    "def neural_network(W1, b1, W2, b2, X, y):\n",
    "    \"\"\"Two-layer neural network.\"\"\"\n",
    "    # Forward pass\n",
    "    hidden = jax.nn.relu(jnp.dot(X, W1) + b1)\n",
    "    output = jax.nn.sigmoid(jnp.dot(hidden, W2) + b2)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    output = jnp.clip(output, 1e-10, 1 - 1e-10)\n",
    "    loss = -jnp.mean(y * jnp.log(output) + (1 - y) * jnp.log(1 - output))\n",
    "    return loss\n",
    "\n",
    "# Generate spiral dataset\n",
    "def make_spiral_data(n_points=100, noise=0.2):\n",
    "    n = n_points // 2\n",
    "    theta = np.linspace(0, 4 * np.pi, n)\n",
    "    \n",
    "    # Class 0\n",
    "    r0 = theta + np.random.randn(n) * noise\n",
    "    x0 = r0 * np.cos(theta)\n",
    "    y0 = r0 * np.sin(theta)\n",
    "    \n",
    "    # Class 1\n",
    "    r1 = theta + np.random.randn(n) * noise\n",
    "    x1 = -r1 * np.cos(theta)\n",
    "    y1 = -r1 * np.sin(theta)\n",
    "    \n",
    "    X = np.vstack([np.column_stack([x0, y0]), np.column_stack([x1, y1])])\n",
    "    y = np.hstack([np.zeros(n), np.ones(n)]).reshape(-1, 1)\n",
    "    \n",
    "    return jnp.array(X), jnp.array(y)\n",
    "\n",
    "# Data\n",
    "np.random.seed(42)\n",
    "X_spiral, y_spiral = make_spiral_data(n_points=200)\n",
    "\n",
    "# Initialize parameters\n",
    "input_dim, hidden_dim, output_dim = 2, 10, 1\n",
    "W1 = jnp.array(np.random.randn(input_dim, hidden_dim) * 0.1)\n",
    "b1 = jnp.zeros((1, hidden_dim))\n",
    "W2 = jnp.array(np.random.randn(hidden_dim, output_dim) * 0.1)\n",
    "b2 = jnp.zeros((1, output_dim))\n",
    "\n",
    "# Compute gradients\n",
    "dnn_dW1 = tangent.grad(neural_network, wrt=(0,))\n",
    "dnn_db1 = tangent.grad(neural_network, wrt=(1,))\n",
    "dnn_dW2 = tangent.grad(neural_network, wrt=(2,))\n",
    "dnn_db2 = tangent.grad(neural_network, wrt=(3,))\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.5\n",
    "n_epochs = 500\n",
    "loss_history = []\n",
    "\n",
    "print(\"Training neural network...\")\n",
    "for epoch in range(n_epochs):\n",
    "    # Compute gradients\n",
    "    grad_W1 = dnn_dW1(W1, b1, W2, b2, X_spiral, y_spiral)\n",
    "    grad_b1 = dnn_db1(W1, b1, W2, b2, X_spiral, y_spiral)\n",
    "    grad_W2 = dnn_dW2(W1, b1, W2, b2, X_spiral, y_spiral)\n",
    "    grad_b2 = dnn_db2(W1, b1, W2, b2, X_spiral, y_spiral)\n",
    "    \n",
    "    # Update\n",
    "    W1 = W1 - learning_rate * grad_W1\n",
    "    b1 = b1 - learning_rate * grad_b1\n",
    "    W2 = W2 - learning_rate * grad_W2\n",
    "    b2 = b2 - learning_rate * grad_b2\n",
    "    \n",
    "    loss = neural_network(W1, b1, W2, b2, X_spiral, y_spiral)\n",
    "    loss_history.append(float(loss))\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: loss = {loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_spiral[:, 0].min() - 1, X_spiral[:, 0].max() + 1\n",
    "y_min, y_max = X_spiral[:, 1].min() - 1, X_spiral[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "grid = jnp.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Compute predictions\n",
    "hidden = jax.nn.relu(jnp.dot(grid, W1) + b1)\n",
    "predictions = jax.nn.sigmoid(jnp.dot(hidden, W2) + b2)\n",
    "Z = predictions.reshape(xx.shape)\n",
    "\n",
    "ax1.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
    "ax1.scatter(X_spiral[y_spiral.flatten() == 0, 0], \n",
    "           X_spiral[y_spiral.flatten() == 0, 1], \n",
    "           c='blue', marker='o', label='Class 0', alpha=0.7, edgecolors='black')\n",
    "ax1.scatter(X_spiral[y_spiral.flatten() == 1, 0], \n",
    "           X_spiral[y_spiral.flatten() == 1, 1], \n",
    "           c='red', marker='s', label='Class 1', alpha=0.7, edgecolors='black')\n",
    "ax1.set_xlabel('Feature 1', fontsize=12)\n",
    "ax1.set_ylabel('Feature 2', fontsize=12)\n",
    "ax1.set_title('Neural Network Decision Boundary', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Loss curve\n",
    "ax2.plot(loss_history, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax2.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Neural Network with JAX & Tangent', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. **Basic Concepts**: How Tangent transforms source code to compute gradients\n",
    "2. **Multi-Backend Support**: Using Tangent with NumPy, TensorFlow, and JAX\n",
    "3. **Advanced Features**: Multiple gradients, result preservation, and caching\n",
    "4. **Visualization**: Plotting functions, gradients, and decision boundaries\n",
    "5. **Real-World Examples**: Linear regression, logistic regression, and neural networks\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Readable**: Generated gradient code is Python you can read and debug\n",
    "- **Efficient**: No tape overhead, compiled gradients run fast\n",
    "- **Flexible**: Works with multiple backends seamlessly\n",
    "- **Educational**: Perfect for learning how autodiff works under the hood\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try Tangent on your own functions\n",
    "- Explore the [documentation](https://github.com/google/tangent)\n",
    "- Implement custom gradient rules for your operations\n",
    "- Compare with other autodiff libraries\n",
    "\n",
    "### Resources\n",
    "\n",
    "- GitHub: https://github.com/google/tangent\n",
    "- Paper: [Automatic differentiation in ML: Where we are and where we should be going](https://arxiv.org/abs/1810.11530)\n",
    "- Tutorial: [Source-to-Source Differentiation](https://github.com/google/tangent/blob/master/docs/walkthrough.md)\n",
    "\n",
    "Happy differentiating! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}