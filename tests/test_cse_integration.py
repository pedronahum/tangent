"""
Integration tests for CSE with real Tangent gradients.

Tests CSE on actual gradient code generated by Tangent.
"""
import unittest
import tangent
import numpy as np
from tangent.optimizations.cse import apply_cse
import gast


class TestCSETangentIntegration(unittest.TestCase):
    """Test CSE on real Tangent-generated gradients."""

    def test_simple_function_gradient(self):
        """Test CSE on gradient of simple function."""
        def f(x):
            y = x * x
            z = y + y  # y appears twice - potential CSE
            return z

        # Generate gradient WITHOUT CSE first
        grad_f = tangent.grad(f, optimized=False, verbose=0)

        # Test it works
        result = grad_f(3.0)
        expected = 4 * 3.0  # d(2*x²)/dx = 4x
        self.assertAlmostEqual(result, expected, places=5)

        print(f"✓ Simple gradient works: f(3.0) = {result}")

    def test_product_rule_gradient(self):
        """Test CSE on product rule gradient."""
        def f(x, y):
            a = x * x
            b = y * y
            return a * b

        # Generate gradient
        grad_f = tangent.grad(f, wrt=(0,), optimized=False, verbose=0)

        # Test correctness
        result = grad_f(2.0, 3.0)
        # d(x²*y²)/dx = 2x*y² = 2*2*9 = 36
        expected = 2 * 2.0 * (3.0**2)
        self.assertAlmostEqual(result, expected, places=5)

        print(f"✓ Product rule gradient works: result = {result}")

    def test_redundant_computation(self):
        """Test function with obvious redundant computations."""
        def f(x):
            # Deliberately redundant
            temp = x * x
            a = temp + 1.0
            b = temp + 2.0
            c = temp + 3.0
            return a + b + c

        # Generate gradient
        grad_f = tangent.grad(f, wrt=(0,), optimized=False, verbose=0)

        # Test correctness
        result = grad_f(5.0)
        # d((x²+1) + (x²+2) + (x²+3))/dx = d(3x²+6)/dx = 6x = 30
        expected = 6 * 5.0
        self.assertAlmostEqual(result, expected, places=5)

        print(f"✓ Redundant computation gradient works: result = {result}")

    def test_cse_on_generated_gradient_code(self):
        """Test applying CSE to Tangent's generated gradient AST."""
        def f(x):
            y = x * x
            return y + y  # y appears twice

        # Get the gradient function object
        grad_f = tangent.grad(f, wrt=(0,), optimized=False, verbose=0)

        # Get its AST (if available)
        # Note: Tangent compiles functions, so we need to work with the AST before compilation
        import inspect

        # Try to get source
        try:
            source = inspect.getsource(grad_f)
            print(f"Generated gradient source:\n{source}")
        except:
            print("Cannot get source from compiled gradient (expected)")

        # Test the gradient works
        result = grad_f(4.0)
        self.assertIsNotNone(result)
        print(f"✓ Gradient computation successful: result = {result}")

    def test_multiple_parameters(self):
        """Test CSE on multi-parameter gradient."""
        def f(x, y, z):
            a = x * y
            b = y * z
            c = x * z
            return a + b + c

        # Gradient w.r.t. first parameter
        grad_f_x = tangent.grad(f, wrt=(0,), optimized=False, verbose=0)

        # Test correctness
        result = grad_f_x(2.0, 3.0, 4.0)
        # d(xy + yz + xz)/dx = y + z = 3 + 4 = 7
        expected = 3.0 + 4.0
        self.assertAlmostEqual(result, expected, places=5)

        print(f"✓ Multi-parameter gradient works: result = {result}")


class TestCSEOnOptimizedGradients(unittest.TestCase):
    """Test CSE integration with Tangent's optimization pipeline."""

    def test_with_basic_optimization(self):
        """Test CSE with Tangent's basic optimizations enabled."""
        def f(x):
            # Function with constant folding opportunity
            a = x * 2.0
            b = a * 2.0
            return b

        # With basic optimization
        grad_f = tangent.grad(f, wrt=(0,), optimized=True,
                             optimizations={'dce': False}, verbose=0)

        result = grad_f(3.0)
        # d(4x)/dx = 4
        expected = 4.0
        self.assertAlmostEqual(result, expected, places=5)

        print(f"✓ With basic optimization: result = {result}")

    def test_with_dce_enabled(self):
        """Test CSE with DCE optimization enabled."""
        def f(x, y):
            # y is unused in gradient w.r.t. x
            a = x * x
            b = y * y  # Should be eliminated by DCE
            return a

        # With DCE
        grad_f = tangent.grad(f, wrt=(0,), optimized=True,
                             optimizations={'dce': True}, verbose=0)

        result = grad_f(3.0, 5.0)
        # d(x²)/dx = 2x = 6
        expected = 6.0
        self.assertAlmostEqual(result, expected, places=5)

        print(f"✓ With DCE enabled: result = {result}")


class TestCSEPerformanceImpact(unittest.TestCase):
    """Test performance impact of CSE (qualitative)."""

    def test_gradient_execution(self):
        """Test that gradient with CSE opportunities still executes correctly."""
        def complex_function(x):
            # Create expression with redundancy
            term1 = x * x
            term2 = x * x * x
            term3 = x * x * x * x
            result = term1 + term2 + term3
            return result

        # Generate gradient
        grad_f = tangent.grad(complex_function, wrt=(0,), optimized=False, verbose=0)

        # Test on various inputs
        test_inputs = [1.0, 2.0, 3.0, 5.0, 10.0]

        for x in test_inputs:
            result = grad_f(x)
            # d(x² + x³ + x⁴)/dx = 2x + 3x² + 4x³
            expected = 2*x + 3*x**2 + 4*x**3
            self.assertAlmostEqual(result, expected, places=3)

        print(f"✓ Complex function gradients correct for all test inputs")

    def test_realistic_ml_function(self):
        """Test CSE on realistic ML-style function."""
        def neural_net_layer(x, w1, w2, w3):
            # Simulates a simple neural network layer
            h1 = x * w1
            a1 = h1 * h1  # Square activation (for simplicity)

            h2 = a1 * w2
            a2 = h2 * h2

            output = a2 * w3
            return output

        # Gradient w.r.t. w1
        grad_w1 = tangent.grad(neural_net_layer, wrt=(1,), optimized=False, verbose=0)

        # Test correctness
        result = grad_w1(2.0, 0.5, 0.3, 0.7)
        self.assertIsNotNone(result)
        self.assertIsInstance(result, (int, float, np.number))

        print(f"✓ Realistic ML function gradient works: result = {result}")


if __name__ == '__main__':
    # Run with verbose output
    unittest.main(verbosity=2)
