{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Gallery of Gradients: Tangent's Readable Code\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pedronahum/tangent/blob/master/examples/Gallery_of_Gradients.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## The Killer Feature: Readable Gradient Code\n",
    "\n",
    "Unlike other autodiff libraries that build computation graphs, **Tangent generates pure, readable Python code** for gradients. This means:\n",
    "\n",
    "- ‚úÖ **You can read it** - See exactly how gradients are computed\n",
    "- ‚úÖ **You can debug it** - Step through with a debugger\n",
    "- ‚úÖ **You can learn from it** - Understand automatic differentiation\n",
    "- ‚úÖ **You can optimize it** - Apply standard Python optimization techniques\n",
    "\n",
    "This notebook is a **curated gallery** of gradient code examples, showing how Tangent transforms various Python patterns into their gradient counterparts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import Tangent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tangent (uncomment for Colab)\n",
    "# !pip install git+https://github.com/pedronahum/tangent.git numpy matplotlib\n",
    "\n",
    "import tangent\n",
    "import numpy as np\n",
    "\n",
    "print(f\"‚úì Tangent version: {tangent.__version__ if hasattr(tangent, '__version__') else 'dev'}\")\n",
    "print(f\"‚úì NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1: Simple Polynomial üî¢\n",
    "\n",
    "Let's start with the simplest possible example: a polynomial function.\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x):\n",
    "    \"\"\"Compute f(x) = x^3 + 2x^2 + 3x + 4\"\"\"\n",
    "    return x**3 + 2*x**2 + 3*x + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate gradient function and print the code\n",
    "dpolynomial = tangent.grad(polynomial, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**Forward Pass (top section):**\n",
    "- Tangent stores intermediate results (`x_to_the_3`, `x_to_the_2`, etc.)\n",
    "- These are needed for computing gradients later\n",
    "\n",
    "**Backward Pass (bottom section):**\n",
    "- Gradients flow **backward** through operations\n",
    "- Each operation's gradient is computed using the **chain rule**\n",
    "- Notice the power rule: `3 * x ** 2` for the $x^3$ term\n",
    "\n",
    "**The Math:**\n",
    "$$\\frac{d}{dx}(x^3 + 2x^2 + 3x + 4) = 3x^2 + 4x + 3$$\n",
    "\n",
    "Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2.0\n",
    "gradient = dpolynomial(x)\n",
    "expected = 3*x**2 + 4*x + 3  # 3*4 + 4*2 + 3 = 23\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"Computed gradient: {gradient}\")\n",
    "print(f\"Expected gradient: {expected}\")\n",
    "print(f\"Match: {np.isclose(gradient, expected)} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: For Loop üîÑ\n",
    "\n",
    "This is where things get interesting! Let's see how Tangent handles a for loop.\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_powers(x, n=5):\n",
    "    \"\"\"Compute sum: x^1 + x^2 + x^3 + ... + x^n\"\"\"\n",
    "    result = 0.0\n",
    "    for i in range(1, n+1):\n",
    "        result += x ** i\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsum_of_powers = tangent.grad(sum_of_powers, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**The Loop Runs Twice!**\n",
    "1. **Forward Pass** (top): Loop runs forward, storing intermediate values\n",
    "2. **Backward Pass** (bottom): Loop runs **in reverse**, accumulating gradients!\n",
    "\n",
    "**Why Reverse?**\n",
    "- Gradients must flow backward through operations\n",
    "- Later operations depend on earlier ones\n",
    "- Running backward ensures correct gradient accumulation\n",
    "\n",
    "**Key Insight:**\n",
    "```python\n",
    "# Forward: result += x ** i\n",
    "# Backward: bx += i * x ** (i-1) * bresult\n",
    "```\n",
    "\n",
    "This is the **chain rule** in action!\n",
    "\n",
    "Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2.0\n",
    "gradient = dsum_of_powers(x, n=5)\n",
    "\n",
    "# d/dx(x + x^2 + x^3 + x^4 + x^5) = 1 + 2x + 3x^2 + 4x^3 + 5x^4\n",
    "expected = sum(i * x**(i-1) for i in range(1, 6))\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"Computed gradient: {gradient}\")\n",
    "print(f\"Expected gradient: {expected}\")\n",
    "print(f\"Match: {np.isclose(gradient, expected)} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 3: While Loop üåÄ\n",
    "\n",
    "While loops are even more interesting because the iteration count depends on runtime values.\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_series(x, threshold=0.001):\n",
    "    \"\"\"Compute sum: x + x^2 + x^3 + ... until term < threshold\"\"\"\n",
    "    result = 0.0\n",
    "    term = x\n",
    "    power = 1\n",
    "    \n",
    "    while term > threshold:\n",
    "        result += term\n",
    "        power += 1\n",
    "        term = x ** power\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgeometric_series = tangent.grad(geometric_series, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**The Challenge:**\n",
    "- While loops don't have a fixed iteration count\n",
    "- Number of iterations depends on input value\n",
    "\n",
    "**Tangent's Solution:**\n",
    "1. **Stack-based tape recording** during forward pass\n",
    "2. Values are pushed onto a stack each iteration\n",
    "3. During backward pass, values are popped in reverse order\n",
    "4. This ensures correct gradient computation regardless of iteration count!\n",
    "\n",
    "**Notice:**\n",
    "- `_stack` variables for recording loop history\n",
    "- `push` operations in forward pass\n",
    "- `pop` operations in backward pass\n",
    "\n",
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.5\n",
    "gradient = dgeometric_series(x, threshold=0.001)\n",
    "\n",
    "# For small threshold, this approaches d/dx(x/(1-x)) = 1/(1-x)^2\n",
    "expected_approx = 1.0 / (1.0 - x)**2\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"Computed gradient: {gradient}\")\n",
    "print(f\"Expected (approx): {expected_approx}\")\n",
    "print(f\"Close enough: {abs(gradient - expected_approx) < 0.1} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 4: Conditional Logic üîÄ\n",
    "\n",
    "How does Tangent handle if/else branches?\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def relu_like(x):\n    \"\"\"A ReLU-like activation: f(x) = x^2 if x > 0 else -x\"\"\"\n    if x > 0:\n        result = x ** 2\n    else:\n        result = -x\n    return result"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drelu_like = tangent.grad(relu_like, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**Forward Pass:**\n",
    "- The condition `x > 0` is evaluated\n",
    "- Only ONE branch executes\n",
    "- The return value is stored\n",
    "\n",
    "**Backward Pass:**\n",
    "- **The same branch** that executed forward must execute backward\n",
    "- Tangent uses the saved condition result\n",
    "- Gradient computed only for the executed path\n",
    "\n",
    "**The Math:**\n",
    "$$\\frac{df}{dx} = \\begin{cases} 2x & \\text{if } x > 0 \\\\ -1 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Let's verify both branches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive case\n",
    "x_pos = 3.0\n",
    "grad_pos = drelu_like(x_pos)\n",
    "expected_pos = 2 * x_pos  # d/dx(x^2) = 2x\n",
    "\n",
    "print(\"Positive branch (x > 0):\")\n",
    "print(f\"  x = {x_pos}\")\n",
    "print(f\"  Gradient: {grad_pos}\")\n",
    "print(f\"  Expected: {expected_pos}\")\n",
    "print(f\"  Match: {np.isclose(grad_pos, expected_pos)} ‚úì\")\n",
    "\n",
    "# Negative case\n",
    "x_neg = -2.0\n",
    "grad_neg = drelu_like(x_neg)\n",
    "expected_neg = -1.0  # d/dx(-x) = -1\n",
    "\n",
    "print(\"\\nNegative branch (x ‚â§ 0):\")\n",
    "print(f\"  x = {x_neg}\")\n",
    "print(f\"  Gradient: {grad_neg}\")\n",
    "print(f\"  Expected: {expected_neg}\")\n",
    "print(f\"  Match: {np.isclose(grad_neg, expected_neg)} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 5: NumPy Array Operations üìä\n",
    "\n",
    "Let's see how Tangent handles arrays and reductions.\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weights):\n",
    "    \"\"\"Compute weighted sum: sum(x * weights)\"\"\"\n",
    "    return np.sum(x * weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dweighted_sum = tangent.grad(weighted_sum, wrt=(0,), verbose=1)  # Gradient w.r.t. x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "1. **Element-wise multiply** `x * weights`:\n",
    "   - Gradient of multiply: each gradient gets the other operand\n",
    "   - $\\frac{\\partial}{\\partial x}(x \\cdot w) = w$\n",
    "\n",
    "2. **Sum reduction** `np.sum(...)`:\n",
    "   - Forward: Many values ‚Üí One value (reduction)\n",
    "   - Backward: One gradient ‚Üí Many gradients (broadcast)\n",
    "   - Tangent uses `unreduce` to reverse the reduction\n",
    "\n",
    "**Broadcasting Magic:**\n",
    "- `unreduce` spreads the scalar gradient back to match the input shape\n",
    "- This handles arrays of any shape automatically!\n",
    "\n",
    "Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 2.0, 3.0])\n",
    "weights = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "gradient = dweighted_sum(x, weights)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"weights = {weights}\")\n",
    "print(f\"Gradient w.r.t. x: {gradient}\")\n",
    "print(f\"Expected: weights = {weights}\")\n",
    "print(f\"Match: {np.allclose(gradient, weights)} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 6: Nested Function Calls üì¶\n",
    "\n",
    "Let's see how Tangent handles function composition.\n",
    "\n",
    "### Original Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: œÉ(x) = 1 / (1 + exp(-x))\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def neural_layer(x, w, b):\n",
    "    \"\"\"Simple neural layer: œÉ(w*x + b)\"\"\"\n",
    "    return sigmoid(w * x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dneural_layer = tangent.grad(neural_layer, wrt=(1,), verbose=1)  # Gradient w.r.t. w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**Function Inlining:**\n",
    "- Tangent **inlines** the `sigmoid` function call\n",
    "- The gradient code contains the full computation\n",
    "- No separate gradient function for `sigmoid` needed!\n",
    "\n",
    "**Chain Rule:**\n",
    "- Outer function: `sigmoid(z)` where `z = w*x + b`\n",
    "- Inner function: `w*x + b`\n",
    "- Gradient: $\\frac{df}{dw} = \\frac{df}{dz} \\cdot \\frac{dz}{dw} = \\sigma'(z) \\cdot x$\n",
    "\n",
    "**Sigmoid Gradient:**\n",
    "- $\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$\n",
    "- Notice how Tangent reuses the forward result!\n",
    "\n",
    "Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w, b = 2.0, 0.5, 0.1\n",
    "gradient = dneural_layer(x, w, b)\n",
    "\n",
    "# Manual computation\n",
    "z = w * x + b\n",
    "sig = sigmoid(z)\n",
    "expected = sig * (1 - sig) * x  # Chain rule: œÉ'(z) * dz/dw\n",
    "\n",
    "print(f\"x={x}, w={w}, b={b}\")\n",
    "print(f\"Gradient: {gradient}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Match: {np.isclose(gradient, expected)} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 7: Matrix Operations üî¢\n",
    "\n",
    "Let's see Tangent handle the new colon slice feature!\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_row_norm(A):\n",
    "    \"\"\"Compute L2 norm of first row: ||A[0, :]||^2\"\"\"\n",
    "    row = A[0, :]  # Extract first row using colon slice\n",
    "    return np.sum(row ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Gradient Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix_row_norm = tangent.grad(matrix_row_norm, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**Colon Slice Magic:**\n",
    "- `A[0, :]` is converted to `A[0, slice(None, None, None)]`\n",
    "- Tangent creates a slice object for the `:` notation\n",
    "- Gradients flow back only to the selected row!\n",
    "\n",
    "**Gradient Routing:**\n",
    "- Forward: Select specific elements (row 0)\n",
    "- Backward: Gradient goes only to those elements\n",
    "- Other elements get zero gradient\n",
    "\n",
    "**The Math:**\n",
    "- For $f(A) = \\sum_i A_{0,i}^2$\n",
    "- $\\frac{\\partial f}{\\partial A_{0,i}} = 2 A_{0,i}$\n",
    "- $\\frac{\\partial f}{\\partial A_{j,i}} = 0$ for $j \\neq 0$\n",
    "\n",
    "Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.0, 2.0, 3.0],\n",
    "              [4.0, 5.0, 6.0]])\n",
    "\n",
    "gradient = dmatrix_row_norm(A)\n",
    "\n",
    "# Expected: 2*A[0,:] in first row, zeros elsewhere\n",
    "expected = np.zeros_like(A)\n",
    "expected[0, :] = 2 * A[0, :]\n",
    "\n",
    "print(f\"A =\\n{A}\")\n",
    "print(f\"\\nGradient =\\n{gradient}\")\n",
    "print(f\"\\nExpected =\\n{expected}\")\n",
    "print(f\"\\nMatch: {np.allclose(gradient, expected)} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 8: Optimization Comparison ‚ö°\n",
    "\n",
    "Tangent can optimize the generated gradient code. Let's see the difference!\n",
    "\n",
    "### Original Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_function(x):\n",
    "    \"\"\"A function with repeated subexpressions\"\"\"\n",
    "    a = x * x\n",
    "    b = a + x\n",
    "    c = a * 2  # Reuses 'a'\n",
    "    return b + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unoptimized Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"UNOPTIMIZED GRADIENT CODE\")\n",
    "print(\"=\" * 70)\n",
    "dcomplex_unopt = tangent.grad(complex_function, optimized=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZED GRADIENT CODE\")\n",
    "print(\"=\" * 70)\n",
    "dcomplex_opt = tangent.grad(complex_function, optimized=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What's Happening?\n",
    "\n",
    "**Optimizations Applied:**\n",
    "\n",
    "1. **Dead Code Elimination (DCE)**\n",
    "   - Removes unused intermediate variables\n",
    "   - Eliminates calculations that don't affect the output\n",
    "\n",
    "2. **Common Subexpression Elimination (CSE)**\n",
    "   - Identifies repeated calculations\n",
    "   - Computes them once, reuses result\n",
    "\n",
    "3. **Algebraic Simplification**\n",
    "   - Simplifies mathematical expressions\n",
    "   - E.g., `x * 1` ‚Üí `x`, `x + 0` ‚Üí `x`\n",
    "\n",
    "**Performance Impact:**\n",
    "- Fewer operations = faster execution\n",
    "- Less memory usage\n",
    "- **Same mathematical result!**\n",
    "\n",
    "Let's verify they're equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3.0\n",
    "grad_unopt = dcomplex_unopt(x)\n",
    "grad_opt = dcomplex_opt(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"Unoptimized gradient: {grad_unopt}\")\n",
    "print(f\"Optimized gradient:   {grad_opt}\")\n",
    "print(f\"Same result: {np.isclose(grad_unopt, grad_opt)} ‚úì\")\n",
    "print(f\"\\nOptimizations don't change correctness, only performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Why Readable Gradients Matter üéØ\n",
    "\n",
    "### 1. **Educational Value** üéì\n",
    "- See exactly how automatic differentiation works\n",
    "- Understand the chain rule in practice\n",
    "- Learn how loops and conditionals are differentiated\n",
    "\n",
    "### 2. **Debugging Power** üêõ\n",
    "- Step through gradient code with a debugger\n",
    "- Set breakpoints in gradient computation\n",
    "- Print intermediate gradient values\n",
    "\n",
    "### 3. **Performance Optimization** ‚ö°\n",
    "- Profile gradient code like any Python function\n",
    "- Apply standard optimization techniques\n",
    "- Understand computational bottlenecks\n",
    "\n",
    "### 4. **Trust and Verification** ‚úÖ\n",
    "- Verify gradient correctness by inspection\n",
    "- No \"black box\" computation graphs\n",
    "- See exactly what Tangent generates\n",
    "\n",
    "### 5. **Customization** üîß\n",
    "- Modify generated code if needed\n",
    "- Add custom logic to gradients\n",
    "- Integrate with existing code seamlessly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next? üöÄ\n",
    "\n",
    "Now that you've seen the gallery, try these:\n",
    "\n",
    "1. **Experiment with your own functions**\n",
    "   - Write a function\n",
    "   - Call `tangent.grad(your_function, verbose=1)`\n",
    "   - Study the generated code!\n",
    "\n",
    "2. **Check out other examples**\n",
    "   - [Building Energy Optimization](Building_Energy_Optimization_with_Tangent.ipynb)\n",
    "   - [Tangent Tutorial](../notebooks/tangent_tutorial.ipynb)\n",
    "\n",
    "3. **Read the documentation**\n",
    "   - [Python Feature Support](../docs/features/PYTHON_FEATURE_SUPPORT.md)\n",
    "   - [Error Messages Guide](../docs/features/ERROR_MESSAGES.md)\n",
    "\n",
    "4. **Contribute!**\n",
    "   - Found a bug? [Report it](https://github.com/pedronahum/tangent/issues)\n",
    "   - Have an idea? [Discuss it](https://github.com/pedronahum/tangent/discussions)\n",
    "   - Want to help? [Pull requests welcome](https://github.com/pedronahum/tangent/pulls)!\n",
    "\n",
    "---\n",
    "\n",
    "## Try It Yourself! üíª\n",
    "\n",
    "Use the cell below to experiment with your own functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own function here!\n",
    "def my_function(x):\n",
    "    # TODO: Add your code here\n",
    "    return x ** 2\n",
    "\n",
    "# Generate and inspect the gradient\n",
    "dmy_function = tangent.grad(my_function, verbose=1)\n",
    "\n",
    "# Test it\n",
    "x_test = 5.0\n",
    "print(f\"\\nGradient at x={x_test}: {dmy_function(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è by the Tangent community**\n",
    "\n",
    "**License**: Apache 2.0\n",
    "\n",
    "**Citation:**\n",
    "```bibtex\n",
    "@misc{tangent_gallery,\n",
    "  title={Gallery of Gradients: Readable Gradient Code with Tangent},\n",
    "  author={Tangent Contributors},\n",
    "  year={2025},\n",
    "  url={https://github.com/pedronahum/tangent/tree/master/examples}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}